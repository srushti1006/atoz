{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!apt install tesseract-ocr\n",
        "!apt install libtesseract-dev\n",
        "!pip install Pillow\n",
        "!pip install pytesseract\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Normalize, Resize, ToTensor\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import whisper\n",
        "import tempfile\n",
        "import moviepy.editor as mp\n",
        "\n",
        "# Initialize Midas model for depth estimation\n",
        "def load_midas_model():\n",
        "    midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\").eval()\n",
        "    return midas\n",
        "\n",
        "# Depth estimation function\n",
        "def estimate_depth(img, midas, transform):\n",
        "    img_input = transform(Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        depth_map = midas(img_input).squeeze().cpu().numpy()\n",
        "    return depth_map\n",
        "\n",
        "# Get transform function for Midas\n",
        "def get_transform():\n",
        "    return Compose([Resize((384, 384)), ToTensor(), Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
        "\n",
        "# OCR function using Tesseract\n",
        "def extract_text_with_ocr(frame):\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    text = pytesseract.image_to_string(gray_frame)\n",
        "    return text\n",
        "\n",
        "# Extract audio from video and transcribe it\n",
        "def transcribe_audio_from_video(video_path):\n",
        "    # Load Whisper model\n",
        "    model = whisper.load_model(\"large\")\n",
        "    # Extract audio\n",
        "    video_clip = mp.VideoFileClip(video_path)\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".mp3\") as temp_audio:\n",
        "        video_clip.audio.write_audiofile(temp_audio.name)\n",
        "        result = model.transcribe(temp_audio.name)\n",
        "        return result[\"text\"]\n",
        "\n",
        "# Generate Amazon listing from captions, transcriptions, OCR text, and dimensions\n",
        "def generate_amazon_listing(captions, transcription, ocr_texts, dimensions):\n",
        "    title = captions[0] if captions else \"Product Title\"\n",
        "    description = f\"{title}. {' '.join(captions[1:])}\"\n",
        "    key_features = [f\"Dimension Range: {dim[0]:.2f}m to {dim[1]:.2f}m\" for dim in dimensions]\n",
        "    listing = {\n",
        "        \"title\": title,\n",
        "        \"description\": description,\n",
        "        \"key_features\": key_features,\n",
        "        \"additional_text\": \" \".join(ocr_texts),\n",
        "        \"transcription_summary\": transcription[:150] if transcription else \"No transcription available\"\n",
        "    }\n",
        "    return listing\n",
        "\n",
        "# Video analysis function\n",
        "def analyze_video(video_path, analyze_audio=True, frame_interval=2, ocr_interval=2):\n",
        "    midas = load_midas_model()\n",
        "    transform = get_transform()\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Load BLIP model and processor for image captioning\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n",
        "\n",
        "    captions = []\n",
        "    dimensions = []\n",
        "    ocr_texts = []\n",
        "\n",
        "    for i in range(0, frame_count, frame_interval * fps):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Caption the frame\n",
        "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        prompt_inputs = processor(image, text=\"Describe the product in detail.\", return_tensors=\"pt\").to(\"cuda\")\n",
        "        output = model.generate(**prompt_inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2)\n",
        "        caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "        captions.append(caption)\n",
        "\n",
        "        # Depth estimation for dimensions\n",
        "        depth_map = estimate_depth(frame, midas, transform)\n",
        "        depth_range = (depth_map.min(), depth_map.max())\n",
        "        dimensions.append(depth_range)\n",
        "\n",
        "        # Perform OCR on additional frames\n",
        "        if i % (ocr_interval * fps) == 0:\n",
        "            ocr_text = extract_text_with_ocr(frame)\n",
        "            if ocr_text:\n",
        "                ocr_texts.append(ocr_text)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Transcribe audio if specified\n",
        "    transcription = transcribe_audio_from_video(video_path) if analyze_audio else None\n",
        "\n",
        "    # Generate final Amazon listing\n",
        "    listing = generate_amazon_listing(captions, transcription, ocr_texts, dimensions)\n",
        "\n",
        "    print(\"\\nGenerated Amazon Listing:\")\n",
        "    print(\"Title:\", listing[\"title\"])\n",
        "    print(\"Description:\", listing[\"description\"])\n",
        "    print(\"Key Features:\", listing[\"key_features\"])\n",
        "    print(\"Additional Text:\", listing[\"additional_text\"])\n",
        "    print(\"Transcription Summary:\", listing[\"transcription_summary\"])\n",
        "\n",
        "    return listing\n",
        "\n",
        "# Run the analysis with user choice for audio analysis\n",
        "video_path = \"/content/hp.mp4\"  # Update with your video path\n",
        "analyze_audio = False  # Set to False if you want to skip audio analysis\n",
        "analyze_video(video_path, analyze_audio=analyze_audio)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urzvtm3yyq62",
        "outputId": "bf6ff65e-3b7d-4736-c231-6b0a5ac36571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-r64sx17g\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-r64sx17g\n",
            "  Resolved https://github.com/openai/whisper.git to commit 5979f03701209bb035a0a466f14131aeb1116cbb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.8.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.4.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights:  None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/rwightman_gen-efficientnet-pytorch_master\n"
          ]
        }
      ]
    }
  ]
}